{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando las librearias necesarias para trabajr con Langchain\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una manera limpia de manejar las API Keys es usando la libreria .env\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leyendo nuestro limpio dataset a un pandas dataframe llamado books\n",
    "import pandas as pd\n",
    "books = pd.read_csv('books_cleaned.csv')\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora es momento de construir nuestro vector search\n",
    "# el metodo TextLoader de Langchain no trabaja con Pandas Dataframes, por lo que se tiene que guardar\n",
    "# los tag descriptions solo en un archivo de texto que estén separados por saltos de lineas, sin index y sin header\n",
    "books['tagged_description'].to_csv('tagged_description.txt',sep=\"\\n\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con las lineas de abajo se busca cargar el texto de nuestro archivo de texto y dividirlo en documentos individuales\n",
    "# 1. Cargamos el texto de nuestro archivo de texto\n",
    "# 2. Dividimos el texto en documentos individuales: Con chunk_size=0 y chunk_overlap=0, estamos diciendo que cada documento es una linea\n",
    "# 3. Cada documento es un tag description\n",
    "\n",
    "# Carga el texto de nuestro archivo de texto a un objeto de tipo Document\n",
    "raw_documents = TextLoader(\"tagged_description.txt\", encoding=\"utf-8\").load() \n",
    "#raw_documents = TextLoader(\"tagged_description.txt\").load()  # Cargamos el texto de nuestro archivo de texto\n",
    "text_splitter = CharacterTextSplitter(chunk_size=0, chunk_overlap=0, separator=\"\\n\")\n",
    "# Esta linea de codigo con chunk_size=0 indica que cada linea es un documento, y con chunk_overlap=0 indica \n",
    "# que no hay overlap entre documentos es decir que no hay palabras que se repitan en dos documentos\n",
    "documents = text_splitter.split_documents(raw_documents)  # Con esta linea de codigo separamos el texto en documentos individuales\n",
    "# Explciando lo que hacen las lineas de arriba\n",
    "# 1. Cargamos el texto de nuestro archivo de texto\n",
    "# 2. Dividimos el texto en documentos individuales: Con chunk_size=0 y chunk_overlap=0, estamos diciendo que cada documento es una linea\n",
    "# 3. Cada documento es un tag description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0] # Mostramos el primer documento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora vamos a crear los document embeddings y guardarlos en un vector database\n",
    "# Usamos el metodo Chroma para crear los embeddings de los documentos y los asiganmos a la variable db_books\n",
    "# Chroma es un metodo de Langchain que permite crear embeddings de documentos\n",
    "# Un embedding es una representacion numerica de un documento, que permite buscar documentos similares entre si\n",
    "# En este caso se esta utilizando el modelo de OpenAI llamado openai-gpt\n",
    "# ************** Teoria de embeddings **************\n",
    "''' Un embedding es una representación numérica (un vector) que captura el contenido de un texto. \n",
    "Permite comparar documentos entre sí midiendo la similitud entre sus vectores.\n",
    "\n",
    "Ejemplo práctico: Supongamos que tienes dos frases:\n",
    "\n",
    "“El gato come pescado.”\n",
    "“El gato come atún.”\n",
    "Al convertirlas en embeddings, obtienes dos vectores (por ejemplo, [0.1, 0.4, 0.8, …] y [0.1, 0.5, 0.7, …]).\n",
    "Si mides la distancia (p. ej. coseno) entre ambos vectores y ves que es pequeña, concluyes que las oraciones \n",
    "son similares en significado. Así, un motor de búsqueda podría encontrar documentos con frases parecidas \n",
    "gracias a esos vectores.\n",
    "'''\n",
    "# 1. Creamos los embeddings de los documentos\n",
    "# 2. Los embeddings son guardados en un vector database llamado db_books\n",
    "# 3. Chroma es un metodo de Langchain que permite crear embeddings de documentos\n",
    "# Los 2 parametros que recibe Chroma son los documentos y el modelo que se va a utilizar para crear los embeddings\n",
    "# En este caso se esta utilizando el modelo de OpenAI llamado openai-gpt\n",
    "db_books = Chroma.from_documents(documents, embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para nuestra primera query busquemos algo especifico\n",
    "query = \"A book to teach childreb about nature\"\n",
    "# Y para obtener las recomendaciones vamos a tomar nuestro vector database y hacer una query\n",
    "docs = db_books.similarity_search(query, k=2)\n",
    "# La linea anterior sirve para hacer una query a nuestro vector database y obtener las recomendaciones, los parametros que recibe son\n",
    "# 1. La query que se va a hacer\n",
    "# 2. El numero de recomendaciones que se van a obtener\n",
    "# En este caso se estan obteniendo 2 recomendaciones\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado que nos da esta bien, porque nos da las descripciones relacionadas a lo que pedimos; pero lo que en realidad tambien buscamos es el Titulo del libro, por lo que necesitamos de algun modo usar estas recomendaciones para filtrar el data frame que contiene todos los libros.\n",
    "Y es ahi donde los ISBN se van a usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear un pandas data frame y en este caso vamos a obtener el primer documento\n",
    "books[ books[\"isbn13\"] == int(docs[0].page_content.split()[0].strip()) ] # Mostramos el primer documento\n",
    "# La linea anterior sirve para filtrar el dataframe y obtener el libro que corresponde a la recomendacion\n",
    "# En este caso se esta filtrando el dataframe por el isbn13 del libro que corresponde a la recomendacion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
